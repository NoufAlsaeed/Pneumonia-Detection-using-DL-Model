{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "name": "FINAL_MLproject (1).ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "kbIzSPVNFVsQ"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import PIL\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import Sequential\n",
        "import cv2 \n",
        "import zipfile\n",
        "import PIL.Image\n",
        "import tensorflow_datasets as tfds\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "from PIL import Image \n",
        "from pathlib import Path \n",
        "from matplotlib import pyplot as plt "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UA2W67fxeH3T"
      },
      "source": [
        "# **Loading the data**\n",
        "\n",
        "First we give the google colab the authority to access our google drive:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "v-3wwqPOXKpi"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aeOUwcp2q6-S"
      },
      "source": [
        "here we start uploading data in our Notebook: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "U8eXUdU45wjJ"
      },
      "source": [
        "  from PIL import Image\n",
        "#Uploading data\n",
        "TRAIN = \"/content/drive/MyDrive/data/chest_xray/train\"\n",
        "TEST =  \"/content/drive/MyDrive/data/chest_xray/test\"\n",
        "VAL = \"/content/drive/MyDrive/data/chest_xray/val\"\n",
        "\n",
        "\n",
        "\n",
        "print('train' , len(os.listdir(TRAIN))) \n",
        "print('test' , len(os.listdir(TEST))) \n",
        "print('val' , len(os.listdir(VAL))) \n",
        "\n",
        "\n",
        "# path of specific image \n",
        "path='/content/drive/MyDrive/data/chest_xray/train/NORMAL/IM-0115-0001.jpeg'\n",
        "\n",
        "#print image\n",
        "plt.imshow(mpimg.imread(path))\n",
        "plt.show()\n",
        "#another way to print images \n",
        "plt.imshow(Image.open(path)) \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "image = Image.open(path)\n",
        "# summarize some details about the image\n",
        "print(image.format)\n",
        "print(image.mode)\n",
        "print(image.size)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4cCdmnP8eRzr"
      },
      "source": [
        "# **Exploring the data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "lbM5oBdeYy1q"
      },
      "source": [
        "path = Path('')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "rYSqJ5wAY2aK"
      },
      "source": [
        "normal_image = path / '/content/drive/MyDrive/data/chest_xray/train/NORMAL/IM-0115-0001.jpeg'\n",
        "pneumonia_image = path / '/content/drive/MyDrive/data/chest_xray/train/PNEUMONIA/person1000_bacteria_2931.jpeg'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "GY4JsSiwZCi3"
      },
      "source": [
        "img = Image.open(normal_image)\n",
        "imgp = Image.open(pneumonia_image)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "pEqod77EZTzY"
      },
      "source": [
        "plt.imshow(img, cmap='bone')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "RDv6BsmiZW2l"
      },
      "source": [
        "plt.imshow(imgp, cmap='bone')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "fpJBdu0TZaG4"
      },
      "source": [
        "plt.imshow(img, cmap='jet')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "1BnM0toyZdnJ"
      },
      "source": [
        "plt.imshow(imgp, cmap='jet')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "2ai68pr3Zg_G"
      },
      "source": [
        "plt.imshow(img, cmap='inferno')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "m8Gk46fyZjsd"
      },
      "source": [
        "plt.imshow(imgp, cmap='inferno')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "HRCb9cl1ZnKr"
      },
      "source": [
        "plt.imshow(img, cmap='gist_ncar')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "HRppgLCWZoov"
      },
      "source": [
        "plt.imshow(imgp, cmap='gist_ncar')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gQrfDRkeA10v"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TuLFDqKdequ-"
      },
      "source": [
        "# **pre-processing data**\n",
        "Data preprocessing is the transformations applied to your data before feeding it to the model by converting the raw data into a clean data set.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ldTdPdWlj7Kw"
      },
      "source": [
        "**what we do?**\n",
        "\n",
        "1-Data augmentation: is the technique of increasing the size of data used for training a model. We apply it on our data to make a better acurracy .\n",
        "\n",
        "data augmentation techniques that we use are: Rotation, shift, shear, zoom, flip, fill.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "N86RGg917pGj"
      },
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "datagen=ImageDataGenerator(\n",
        "    rotation_range=40,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest'\n",
        "\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eQe8l9R5o5la"
      },
      "source": [
        "then we applied the following steps:\n",
        "\n",
        "2-read the picture files through `flow_from_directory`function. \n",
        "\n",
        "3-rescale the pixel values (between 0 and 255) to [0,1] interval(normalization).\n",
        "\n",
        "4-resize image to 350x350."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "CpU-WG9LW7no"
      },
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# All images will be rescaled by 1./255\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "val_datagen = ImageDataGenerator(rescale=1./255)\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=40,\n",
        "    width_shift_range=0.2, \n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2, \n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    )\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        TRAIN,# This is the target directory\n",
        "        target_size=(350, 350), # as large as possible    \n",
        "        batch_size=32,# No. of images to be yielded from the generator per batch.\n",
        "        # Since we use binary_crossentropy loss, we need binary labels\n",
        "        class_mode='binary')# Set “binary” if you have only two classes to predict,\n",
        "validation_generator =test_datagen.flow_from_directory(\n",
        "        VAL,\n",
        "        target_size=(350, 350),\n",
        "        batch_size=32,\n",
        "        class_mode='binary')\n",
        "\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "        TEST,\n",
        "        target_size=(350, 350),\n",
        "        batch_size=32,\n",
        "        class_mode='binary')\n",
        "        #shuffle=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JWG3Tsztkq0s"
      },
      "source": [
        "**here we print the `data_batch` and `labels_batch`:**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ybkFa2Oxybt3"
      },
      "source": [
        "\n",
        "for data_batch, labels_batch in train_generator:\n",
        "    print('data batch shape:', data_batch.shape)\n",
        "    print('labels batch shape:', labels_batch.shape)\n",
        "    break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "taJD4undeaHX"
      },
      "source": [
        "\n",
        "# **Building the model**\n",
        "we chose the type of keras models which is sequential model, sequential is linear stack of layers.\n",
        "\n",
        "building the layers :\n",
        "\n",
        "**step1:** Convolution `layers.Conv2D` A convolution multiplies a matrix of pixels with a filter matrix or ‘kernel’ and sums up the multiplication values.\n",
        "Then the convolution slides over to the next pixel and repeats the same process until all the image pixels have been covered and generate feature maps. \n",
        "\n",
        "**step2:** pooling which is down-sampling, most often in the form of \"max-pooling,\" `layers.MaxPooling2D`\n",
        "where we select a region, and then take the maximum value in that region, and that becomes the new value for the entire region. \n",
        "\n",
        "**then we repeat these two steps three times with increasing the number of nodes on convolution layer to have more details.**\n",
        "\n",
        "**step3:**Flatten `layers.Flatten()` this converts our 3D feature maps to 1D feature vectors.\n",
        "\n",
        "**step4:**Dropout `layers.Dropout`prevent neural networks from Overfitting by reducing the capacity or thinning the network during training.\n",
        "\n",
        "**step5:**Dense `layers.Dense`\n",
        "is the layer that we will use in for our output layer.we use two Dense layers \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "3YPzRZ1qxx97"
      },
      "source": [
        "from keras import layers\n",
        "from keras import models\n",
        "from keras import optimizers\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.utils.vis_utils import plot_model\n",
        "\n",
        "\n",
        "model = models.Sequential() \n",
        "#architecture of the model\n",
        "\n",
        "#step1 #32 is number of nodes in layer#(3,3) is the size of the window \n",
        "model.add(layers.Conv2D(32, (3, 3), activation='relu', #32 number of nodes in the layer.\n",
        "                        input_shape=(350, 350, 3))) #(rows, cols, RBG)\n",
        "                        \n",
        "#step2 #(2,2) is the size of window OR (Kernel size) \n",
        "model.add(layers.MaxPooling2D((2, 2))) \n",
        "                           \n",
        "\n",
        "#another layer of Convolution\n",
        "model.add(layers.Conv2D(64, (3, 3), activation='relu'))  \n",
        "                                                        \n",
        "\n",
        "#another layer of pooling\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "#another layer of Convolution\n",
        "model.add(layers.Conv2D(128, (3, 3), activation='relu')) \n",
        "\n",
        "#another layer of pooling\n",
        "model.add(layers.MaxPooling2D((2, 2))) \n",
        "\n",
        "#step3\n",
        "model.add(layers.Flatten())\n",
        "\n",
        "#step4\n",
        "model.add(layers.Dropout(0.5))\n",
        "\n",
        "#step5\n",
        "model.add(layers.Dense(512, activation='relu')) #521 number of nodes in the layer.\n",
        "model.add(layers.Dense(1, activation='sigmoid')) #1 number of nodes in the layer.\n",
        "\n",
        "#Before training a model, you need to configure the learning process,\n",
        "# which is done via the compile() function.\n",
        "\n",
        "\n",
        "history= model.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam',#The optimizer controls the learning rate  \n",
        "             metrics=['binary_accuracy']\n",
        "              )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6C6aqZmoXIpY"
      },
      "source": [
        "We display layers into two ways:\n",
        "\n",
        "1-Graphical \n",
        "\n",
        "2- Table \n",
        "\n",
        "From these two ways we can observe that each layer’s output becomes the input for the subsequent layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "nlXEnbRkW-S2"
      },
      "source": [
        "plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_yvaVLXvzPNx"
      },
      "source": [
        "Each layer has an output and its shape is shown in the `Output Shape` column. \n",
        "\n",
        "`output shape`: batch_shape + (new_rows, new_cols, filters).\n",
        "\n",
        "`Param`: column shows you the number of parameters that are trained for each layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "jS1ommKFx5wd"
      },
      "source": [
        "model.summary() "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aSN7k6w5Zb3e"
      },
      "source": [
        "Now we will train the model using `model.fit`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "KNKHOLZ-_0V4",
        "outputId": "b27be05c-abbe-407f-cca5-a0627e46a9d7"
      },
      "source": [
        "history=model.fit(\n",
        "    train_generator,\n",
        "    steps_per_epoch=60,\n",
        "    epochs=30,\n",
        "    validation_data=validation_generator,\n",
        "    validation_steps=50\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "60/60 [==============================] - ETA: 0s - loss: 1.3622 - binary_accuracy: 0.6778 WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 50 batches). You may need to use the repeat() function when building your dataset.\n",
            "60/60 [==============================] - 798s 13s/step - loss: 1.3516 - binary_accuracy: 0.6789 - val_loss: 0.5729 - val_binary_accuracy: 0.5625\n",
            "Epoch 2/30\n",
            "60/60 [==============================] - 613s 10s/step - loss: 0.3920 - binary_accuracy: 0.8090\n",
            "Epoch 3/30\n",
            "60/60 [==============================] - 605s 10s/step - loss: 0.3594 - binary_accuracy: 0.8165\n",
            "Epoch 4/30\n",
            "60/60 [==============================] - 576s 10s/step - loss: 0.3613 - binary_accuracy: 0.8363\n",
            "Epoch 5/30\n",
            "60/60 [==============================] - 570s 9s/step - loss: 0.3091 - binary_accuracy: 0.8648\n",
            "Epoch 6/30\n",
            "45/60 [=====================>........] - ETA: 2:18 - loss: 0.2775 - binary_accuracy: 0.8814"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IykAgzIjLUzn"
      },
      "source": [
        "## **Evaluate the model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UD_NrVoN4k-B"
      },
      "source": [
        "#Try to install HDF5 format with this line:\n",
        "\n",
        "# !pip install pyyaml h5py\n",
        "from keras.models import load_model\n",
        "from tensorflow.keras.models import save_model, load_model\n",
        "save_model(\"pneumonia_model.h5\")\n",
        "model = load_model(\"pneumonia_model.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ld8YZ92PLoRF"
      },
      "source": [
        "tf.config.experimental_run_functions_eagerly(True)\n",
        "test_loss, test_accuracy = model.evaluate(test_generator,verbose=0)\n",
        "test_loss = round(test_loss, 6)\n",
        "test_accuracy = round(test_accuracy*100, 3)\n",
        "\n",
        "print('Test Loss: ', test_loss)\n",
        "print('Test Accuracy:', '\\033[0m', test_accuracy, '%\\033[0m')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qT1wmn7IXZe8"
      },
      "source": [
        "# Confusion Matrix\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.metrics import plot_confusion_matrix\n",
        "batch_size=32\n",
        "Y_pred = model.predict(test_generator, 624 // batch_size+1)\n",
        "y_pred = np.argmax(Y_pred, axis=1)\n",
        "print('Confusion Matrix')\n",
        "print(confusion_matrix(test_generator.classes, y_pred))\n",
        "confusion_mtx=confusion_matrix(test_generator.classes, y_pred)\n",
        "#plot_confusion_matrix(estimator=classifier, X = test_generator.classes, y_true = y_pred,labels= range(2), normalize=False)\n",
        "print('Classification Report')\n",
        "target_names = ['Normal', 'Pneumonia']\n",
        "print(classification_report(test_generator.classes, y_pred, target_names=target_names))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iu5f_SMeHZjv"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}